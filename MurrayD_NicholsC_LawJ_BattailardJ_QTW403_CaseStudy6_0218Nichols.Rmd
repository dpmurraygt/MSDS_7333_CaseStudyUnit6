---
title: 'Indoor Positioning Systems: An Exploration Using K-Nearest Neighbors'
author: "Dennis Murray, Jared Law, Julien Battaillard, Cory Nichols"
section: "MSDS 7333-403 - Quantifying the World - Case Study 3 (Unit 6)"
date: "February 20th, 2018"
output: 
  word_document:
    reference_docx: word-styles-reference-01.docx
    fig_caption: yes
---

```{r setup, include=FALSE, echo=FALSE}
dir <- '~/DataScience/SMU_Data_Science/MSDS_QTW/MSDS_7333_CaseStudyUnit6/'
knitr::opts_knit$set(root.dir = dir)
knitr::opts_chunk$set(echo = FALSE)
```

```{r import_libs, include=FALSE, echo=FALSE, cache=TRUE}
library(tidyr)
library(dplyr)
library(fields)
library(lattice)
library(knitr)
library(ggplot2)
```

```{r make, include=FALSE, echo=FALSE, cache=TRUE}
# get offline, online and onlineSummary along with key functions
source('src/MakeFile.R')
```

## Abstract

Real-time location systems (RTLS) are commonly used for identifying the location of an asset inside of a room. These RTLS applications are called indoor positioning systems. In this paper, we investigate the effect of removing an access point on the accuracy of an indoor positioning system developed by Nolan and Temple Lang (1). We also improve upon their location determination methods by implementing a weighted *k*-nearest neighbors model for position identification. Results show an improvement in location prediction when considering a different access point previously unused in the authors' analysis. Additionally, we find greater success using a weighted *k*-nearest neighbors approach for determining location.

## Introduction

Tracking an Amazon package online utilizes a real-time location system to communicate critical location information to consumers. With a simple button press on their phone, users can track their packages instantly. Real-time location systems are also a component in inventory management and logistics. Companies such as Dell Computer pioneered inventory management systems utilizing real-time location systems (Ojo-Osagie 2). 

Real-time location systems (RTLS) automatically track and identify the location of people or objects wirelessly. A classic RTLS example is location tracking of objects in a room. This RTLS application is called an indoor positioning system. Hardware such as bluetooth tags or cell phones are used as mobile beacons to communicate with strategically placed access points around the room. These beacons can be attached to people, other devices, and even robots. A beacon produces a signal that is read by the access points around the room. Each signal provides data used to determine the location of the mobile device within the room. Software applications utilizing triangulation, trilateration or a combination of location determination algorithms actively translate this location data and produce usable interfaces for people to identify the location of a beacon.

Applications for real-time location systems are vast and extend beyond a simple enclosed space. Advances in wireless technologies and proliferation of tracking tags have made real-time location systems ubiquitous in manufacturing, inventory management, and navigation (Winick 3). Identifying the current location of a package is a popular use of RTLS technology. Tracking previous location history and tracing locations to predict future locations are also common tasks. Additionally, active tags can provide critical data on the temperature of a package or the blood sugar level of a patient along with location information.

In this paper, we improve upon previous analysis by Nolan and Temple Lang (1) using indoor wireless signal strength data provided by Mannheim University. These data are generated from a single mobile device at 166 locations on one floor of a multi-story building. Eight orientation angles are considered at each location and 110 readings are taken for each (x,y) location, angle combination. For each reading, seven access points provide signal strength data to the mobile device. 

In their analysis, Nolan and Temple Lang implement an indoor positioning system using signal strength data and a mean-based *k*-nearest neighbors model. The model is used to predict the location of a mobile device using previously unseen signal strength data. The authors drop an access point from their training data set and use six of the seven access points for their *k*-nearest neighbors model. 

We investigate the accuracy of the authors' *k*-nearest neighbors model utilizing the access point previously not considered. We also extend their mean-based  *k*-nearest neighbors approach by implementing a weighted *k*-nearest neighbors model for predicting the location of previously unseen signal strength data.

## Literature Review

Various reader and tag communication protcols are used depending on the technology employed with real-time location systems. For example, radio frequency identification (RFID) often employs ISO 18000, a universal protocol for different RFID frequencies (ISO 4). On the other hand, infrared line-of-sight systems often use affordable hardware and a protocol stack for short range communication (Barker, et al 5). These and other protocols along with associated hardware allow for location data to be passed through objects and provide for precise indoor location identification. 

The techniques used to analyze the position of the tags within each system can also vary. Communication protocols and associated data transmission methods impact analysis tasks for determining the location of a particular target. Though we utilize *k*-nearest neighbors in this paper, other location prediction methods frequently depend on wireless tag communication protocols (Boulos, et al 6). 

Bayesian methods have been implemented for location prediction (Madigan, et al 7). Location information is not necessary for training given prior knowledge about the nature of Wi-Fi signals. Thus, query times for prediction were reduced and location prediction proved much faster than traditional *k*-nearest neighbor methods.

To address inaccuracies and rough calibrations associated with common wireless location modeling, weighted least squares techniques have been developed to improve localization results (Tarrío, et al 8). These techniques outperform standard propogation channel modeling for multiple wireless network types. They also result in very limited overhead compared to standard methods associated with channel modeling and produce more accurate location results. 

Real-time location systems are applied in inventory management, livestock maintenance, and personnel tracking. Healthcare has continued to develop ways to utilize real-time location systems to limit cross-contamination. At the Toronto University Health Network, a real-time location system allows the university to locate personnel and equipment entering a contaminated area. The real-time location system sends an alert if the person or equipment hasn't been decontaminated (Swedberg 9). Another example is a patent filed for tracking livestock that would assist ranchers and farmers in obtaining location information on their herds. This system utilizes the distance value transmitted from the reader and *k*-nearest neighbor analysis to determine location (Chung 10). In the defense industry, contractors assist in moving products from one location to another in their industrial processes to increase safety, efficiency and reduce human overhead (Hidalgo 11). 

Supporting hardware and communication protocols will continue to advance as businesses continue using real-time location systems to improve fulfillment requests with robotics and in warehouse distribution (Ding 12). An array of location determination algorithms currently exist. However, these algorithms are often dependent upon the scope and infrastructure deployment of the real-time location system. Consequently, there is currently no one-size-fits-all approach when it comes localization or location determination.

## Methods

In this paper, we investigate the impact of Nolan and Temple Lang's decision to exclude an access point from their indoor positioning system. We also implement a weighted *k*-nearest neighbors model and compare it to the mean-based *k*-nearest neighbors model they developed. Prior to discussing methods for accomplishing our two analysis objectives, we provide salient findings about the raw data and the transformations taken to prepare the data for analysis.

The raw data is split into a training data set and a test data set. We focus on the training data set for analysis purposes. The training data set contains 151,392 unique lines detailing the (x,y) position and orientation of a mobile device. At each position and orientation combination, 110 signal strength readings are obtained between the mobile device and seven access points. In addition to signal strength readings, variables such as the POSIX time of the reading and the physical MAC address for each access point are also captured.

After excluding extraneous access points, normalizing orientation values, and translating time variables, a data set for analysis is created. The format of this data set can be seen in Table 1.

```{r offline_sample, include=TRUE, echo=FALSE, cache=TRUE, fig.cap=cap, warning=FALSE}

subMacs = names(sort(table(offline$mac), decreasing = TRUE))[1:7]
kable(offline[1:5,], caption="Table 1: Formatted Dataset for Analysis", row.names = FALSE)
cap <- "Table 1: Transformed Offline Dataset Structure"

```

Key exploratory data analysis results help to form assumptions and methods for an indoor positioning system using a *k*-nearest neighbors model. Non-normal, multi-modal signal distributions are found when considering signal strength and the angle of the mobile at each (x,y) location reading. This indicates angle has an effect on signal strength. Figure 2 displays this analysis finding for one access point at a single (x,y) position.

```{r angle_density_plot, include=TRUE, echo=FALSE, cache=TRUE, fig.cap=cap}
densityplot( ~ signal | factor(angle), 
             data = offline,
             subset = posX == 24 & posY == 4 & 
                         mac == "00:0f:a3:39:e1:c0",
             bw = 0.5, plot.points = FALSE)
cap <- "Figure 1: Angle Based Density Plots for 00:0f:a3:39:e1:c0"
```

The transformed data set is subsequently aggregated by (x,y) position, angle, and MAC address. Additional analysis using the aggregated data set shows higher variability for stronger signal strengths. Also, the relationship between signal and distance shows that signal strength weakens as distance increases. This negative relationship is expected and can be seen for each access point in Figure 2.

```{r summary_creation, include=FALSE, echo=FALSE, cache=TRUE}
# create table of summary stats by Location, Angle and AP
offline$posXY = paste(offline$posX, offline$posY, sep = "-") # concat x,y

byLocAngleAP = with(offline, 
                    by(offline, list(posXY, angle, mac), 
                       function(x) x))

# calculate summary statistics, reduce down to single summary line
signalSummary = 
  lapply(byLocAngleAP,            
         function(oneLoc) {
           ans = oneLoc[1, ]
           ans$medSignal = median(oneLoc$signal)
           ans$avgSignal = mean(oneLoc$signal)
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
           })

# bind all the summary lines together
offlineSummary = do.call("rbind", signalSummary)

```

```{r xyplot, include=TRUE, echo=FALSE, cache=TRUE, fig.cap=cap}

# exclude mac at position 2: "00:0f:a3:39:dd:cd" per author's guidance
offlineSummary_orig = subset(offlineSummary, mac != subMacs[2])

# create a mtrix with relevant positions for six access points on floor plan
AP = matrix(c( 7.5, 6.3, 2.5, -.8, 12.8, -2.8,  
                1, 14, 33.5, 9.3, 33.5, 2.8),
            ncol = 2, byrow = TRUE,
            dimnames = list(subMacs[-2], c("x", "y") ))

# relationship between signal stregnth and distance from AP
# distances from locations of the device emitting vs access point receiving
diffs = offlineSummary_orig[ , c("posX", "posY")] - AP[ offlineSummary_orig$mac, ] 

offlineSummary_orig$dist = sqrt(diffs[ , 1]^2 + diffs[ , 2]^2)

# simplified
xyplot(signal ~ dist | factor(substr(mac,10,117)) , 
       data = offlineSummary_orig,
       xlab ="distance"
       )

cap <- "Figure 2: Relationship Between Signal and Distance"
```

To prepare the training and test data sets for use in a *k*-nearest neighbors model, we aggregate records based on (x,y) position. Signal strengths from each access point for each position are mean-aggregated, forming a data set where each record is unique based on (x,y) position. Each position is associated with a vector of six signal strength values. A sample of this format can be seen in Table 2.

```{r knn_training_example, include=TRUE, echo=FALSE, cache=TRUE, fig.cap=cap}

# this is the original reshape function
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY")) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

# build all of this into a funciton
trainSS = reshapeSS(offlineSummary_orig, varSignal = "avgSignal")
kable(trainSS[1:5,1:6], caption="Table 2: Formatted Dataset for KNN Use", row.names = FALSE)
cap <- "Table 2: Aggregate Data for Analysis"
```

Given previous analysis results, we assess the impact of Nolan and Temple Lang's decision to exclude a MAC address on the accuracy of their *k*-nearest neighbors model. We also implement a weighted *k*-nearest neighbors model to predict location inside of the building. 

### Analysis of Extraneous MAC Address

Nolan and Temple Lang use six of seven access points presented in the training data set. They exclude an access point with a MAC address ending in 39:dd:cd. This particular access point resides at roughly the same location as another access point (39:e1:c0) on the building floor.

Prior experience with triangulation of position based on sighting lines in land navigation leads to a general finding that additional measurement points will reduce the area of uncertainty for a specific position. Therefore, it could be hypothesized a similar effect can be achieved for an indoor positioning system that uses signal strength of known-position access points to locate an unknown receiver's physical location.

Visual analysis of the relative signal strength of the two access points reveals a higher degree of certainty for the access point with a MAC address ending in 39:e1:c0. In the visualization below, the top two plots show signal strength for the retained MAC Address access point ending in 39:e1:c0 from the angles 90 degrees and 135 degrees. The bottom plots repeat the exercise, including the access point ending in 39:dd:cd previously removed from the analysis by Nolan and Temple Lang.

```{r surface_compare, include=TRUE, echo=FALSE, cache=TRUE, fig.cap=cap}

compareMacList <- c("00:0f:a3:39:dd:cd", "00:0f:a3:39:e1:c0")

surfaceSS = function(data, mac, angle = 45) {
  require(fields)
  oneAPAngle = data[ data$mac == mac & data$angle == angle, ]
  smoothSS = Tps(oneAPAngle[, c("posX","posY")], 
                 oneAPAngle$avgSignal)
  vizSmooth = predictSurface(smoothSS)
  plot.surface(vizSmooth, type = "C", 
               xlab = mac, ylab = "", xaxt = "n", yaxt = "n")
  points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5) 
}

parCur = par(mfrow = c(2,2), mar = rep(1, 4)) 

mapply(surfaceSS, 
       mac = compareMacList[rep(c(1,2), each=2)], 
       angle = rep(c(90, 135), 2), 
       data = list(data = offlineSummary))

par(parCur)
cap <- "Figure 3: Signal Strength for Two Similar Access Points"
```

To test the decision made by Nolan and Temple Lang, we compare the cross-validation results of *k*-nearest neighbors models with three different training data sets. Formally, we compare the root mean square error between predicted location and actual location.

*Exclusion of the Access Point with MAC Address Ending in 39:dd:cd*

```{r define_funcs, include=TRUE, echo=FALSE, cache=TRUE}
# define critical prediction and CV functions for analysis purposes

# reshape signal strength, ensure we use if (sampleAngle) for CV purposes
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

# get training data for each new obs (including angle logic)

selectTrain = function(angleNewObs, signals = NULL, m = 1){
  refs = seq(0, by = 45, length  = 8)
  nearestAngle = roundOrientation(angleNewObs)
  
  if (m %% 2 == 1) 
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {
    m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(angleNewObs - nearestAngle) > -1) 
      angles = angles[ -1 ]
    else 
      angles = angles[ -m ]
  }
  angles = angles + nearestAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles) 
  
  offlineSubset = signals[ signals$angle %in% angles, ]
  reshapeSS(offlineSubset, varSignal = "avgSignal")
}

# KNN: FIND NEIGHBORS - AVG BASED
findNN = function(newSignal, trainSubset) {
  diffs = apply(trainSubset[ , 4:9], 1, 
                function(x) x - newSignal) # this inverts, places x,y on cols when called as.numeric
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) ) # this gets our distance (euclidean)
  closest = order(dists) # orders our distances in ascending
  return(trainSubset[closest, 1:3 ]) # returns the subset from training with closest distances, gives xy ID, x, y
}

# PREDXY: takes an input of newSignals, their angles, and returns the estimated positions
predXY = function(newSignals, newAngles, trainData, 
                  numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)
    closeXY[[i]] = findNN(newSignal = as.numeric(newSignals[i, ]),
                           trainSS)
  }

  estXY = lapply(closeXY, function(x)
                            sapply(x[ , 2:3], 
                                    function(x) mean(x[1:k])))
  estXY = do.call("rbind", estXY)
  return(estXY)
}

# ERROR CALCULATION
calcError = 
function(estXY, actualXY) 
   sum( rowSums( (estXY - actualXY)^2) )


# WKNN: WEIGHTED K NEIGHBORS
findWtdNN = function(newSignal, trainSubset) {
  diffs = apply(trainSubset[ , 4:9], 1, 
                function(x) x - newSignal) 
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) ) 
  closest = order(dists) # orders distances ascending
  closeXY = trainSubset[closest, 1:3 ]
  weight = as.numeric(1/dists[closest]) 
  return(cbind(closeXY, weight)) 
}



# pull cross validation process into a repeatable function
cross_validate = function(train_full, train_summary, K=20, folds=11, 
                          keepVars = c("posXY", "posX","posY", "orientation", "angle"),
                          wtd=FALSE) {
set.seed(25)
permuteLocs = sample(unique(train_summary$posXY))
permuteLocs = matrix(permuteLocs, ncol = folds, 
                     nrow = floor(length(permuteLocs)/folds))

onlineCVSummary = reshapeSS(train_full, keepVars = keepVars, sampleAngle = TRUE)

err = rep(0, K)

for (j in 1:folds) {
  onlineFold = subset(onlineCVSummary, 
                      posXY %in% permuteLocs[ , j])
  offlineFold = subset(train_summary,
                       posXY %in% permuteLocs[ , -j])
  actualFold = onlineFold[ , c("posX", "posY")]
  
    for (k in 1:K) {
      if(wtd == TRUE){
                       estFold = predXYwtd(newSignals = onlineFold[ , 6:11],
                                 newAngles = onlineFold[ , 4], 
                                 offlineFold, numAngles = 3, k = k)
                       err[k] = err[k] + calcError(estFold, actualFold)
      
                     } else {
                       estFold = predXY(newSignals = onlineFold[ , 6:11],
                                        newAngles = onlineFold[ , 4], 
                                        offlineFold, numAngles = 3, k = k)
                       err[k] = err[k] + calcError(estFold, actualFold)
                     }
                    }
                   }

rmseMin = min(err)
kMin = which(err == rmseMin)[1]

return(list(rmseMin, kMin, err))
}

```

```{r cross_validation_orig, include=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}

# identify dataset for training (exclude MAC per authors guidance)
# get minimum error and optimal K

offline_orig = offline[ offline$mac != "00:0f:a3:39:dd:cd", ]
results = cross_validate(offline_orig, offlineSummary_orig, K=20, wtd=FALSE)
print(paste('results of original',results))

```

Model results are established via cross-validation considering 11 folds and 15 locations sampled randomly for each fold. We establish a random seed to ensure consistent results for each training data set considered.

Repeating Nolan and Temple Lang's analysis by excluding the access point with a physical address ending in 39:dd:cd yields an optimal root mean square error of 1038.5 using six neighbors. The worst root mean square error of 1515.0 was produced by a *k*-nearest neighbors model considering one neighbor.

*Exclusion of the Access Point with MAC Address Ending in 39:e1:c0*
``` {r cross_validation_new, include=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}

offline_new = offline[ offline$mac != "00:0f:a3:39:e1:c0", ]
offlineSummary_new = offlineSummary[offlineSummary$mac != "00:0f:a3:39:e1:c0", ]

results_new = cross_validate(offline_new, offlineSummary_new, K=20, wtd=FALSE)
print(paste('results of new',results_new))
```

Next, we build another model excluding the access point with a physical network location ending in 39:e1:c0. Instead, we include the access point 39:dd:cd previously not considered by Nolan and Temple Lang. Cross-validating our *k*-nearest neighbors model utilizing the access point previously not considered results in a minimum error of 935.6 given seven neighbors.

Reviewing the previous signal strength visualization, we hypothesize that the abrupt drop-off in signal strength offers a more discriminatory set of data to model against than an access point with very slowly decaying signal strength relative to distance. We investigate an mean-based *k*-nearest neighbors model utilizing both access points next.

*Inclusion of Both Access Points*
``` {r cross_validation_combined, include=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}

# modify author's reshapeSS static function to include both MACs
# change this back to 6 cols for KNN investigation

reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 7,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

results_combined = cross_validate(offline, offlineSummary, K=20, wtd=FALSE)
print(paste('results of combined',results_combined))
```

Including both MAC addresses results in a minimum root mean square error of 1100.0 using four neighbors. This result implies a model considerng both access points underperforms relative to models including only one of the two access points. We formally explore comparisons in the results section.

### Weighted K-Nearest Neighbors Implementation

Nolan and Temple Lang implement a *k*-nearest neighbors model utilizing the mean of *k* known neighbors in the training data to predict the position of previously unseen records. These records contain signal strengths from six access points on the building floor.

Prior to model fitting, it is determined that the orientation of a reading has an impact on signal strength. For instance, when considering an x,y position of (2,12), MAC address ending in b1:97:90 shows signal strength variability differences based on the orientation considered. 

```{r eda_signalstr, include=TRUE, echo=FALSE, cache=TRUE, fig.cap=cap}
# INVESTIGATING SIGNAL STRENGTH
bwplot(signal ~ factor(angle) | mac, 
       data = offline, 
       subset = posX == 2 & posY == 12 
                & mac != "00:0f:a3:39:dd:cd", 
       layout = c(2,3))
cap <- "Figure 4: Angle Influence on Signal Strength for (x,y) Position (2,12)"

```

The authors identify an angle parameter as part of their *k*-nearest neighbors model. This parameter allows the model to account for multiple angles when considering training set records for predicting the location of new observations. While we do not explore the angle parameter in our analysis, we note that three angles are considered when building the training data used for the *k*-nearest neighbors model. Thus, for every new prediction of location, a training set is created using three orientations. To maintain consistency, we also preserve three orientations in the implementation of a weighted *k*-nearest neighbors model. This parameter value reduces bias and allows for multiple angles to influence the mean signal strength used for each observation in the training dataset.

Implementing a weighted *k*-nearest neighbors model requires weighting each training observation associated with the new location we wish to predict. New location in this task means a previously unseen vector of six signal strengths. Training observations closer to the new observation are weighted heavily, while training observations further away have smaller weights. The distance between any new observation and observations in the training data is calculated with Euclidean distance:

Equation 1: Euclidean Distance

$\sqrt{(S_1^*-S_1)^2+...+(S_6^*-S_6)^2}$

Signal strengths (*S*) are considered for all six access points when calculating distance. This results in a distance calculation for each training observation given each new observation. 

These distances are sorted in ascending order. Weights are calculated based on the choice of *k*, or the number of observations in the training set to consider when predicting the location for the new observation. Thus, the *k* neighbors are training observations taken from the sorted list of distances. In the example of five neighbors, we utilize the five closest training observations based on Euclidean distance to predict the location of a new observation. Each of the five training observations used for prediction are weighted according to Equation 2.

Equation 2: Nearest Neighbors Weighting

$\frac{1/di}{\sum_{i=1}^{k}{1/di}}$

For each of the *k* training observations, the numerator is represented as one over the distance between the training observation and the new observation. The denominator is the sum of each numerator for the *k* observations we are considering. The first neighbor (k=1) is the closest to the new observation, therefore the weight associated with the first neighbor is always the largest.

Given previous weighting logic, we are able to form a vector of individual weights for each observation in the training dataset used for the *k*-nearest neighbors model. These weights are subsequently multiplied by the x and y locations of each *k* neighbor and summed to predict a weighted (x,y) position for the new observation. To ensure proper predictions, only the weights for each *k* neighbor is considered. All other neighbors assume a weight of zero. Given a new observation, the weight vector can be seen for three neighbors in Table 3.

```{r weighted_vec_ex, include=TRUE, echo=FALSE, cache=TRUE}

# re-establish standard reshapeSS
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

# weighted prediction vector results, adjusted
predXYwtd = function(newSignals, newAngles, trainData, 
                     numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)
    base = findWtdNN(newSignal = as.numeric(newSignals[i, ]), trainSS) # get matrix of x,y, numerator for weights
    wts = append(base[1:k, 4]/sum(base[1:k, 4]), rep(0, nrow(base)-k))  # calculate weights based on K, append zero array for delta of len-k
  }
  return(cbind(base[,2:3], wts))
}

onlineSummary_orig = onlineSummary[, !(names(onlineSummary) %in% "00:0f:a3:39:dd:cd")]
weight_ex <- predXYwtd(onlineSummary_orig[1,6:11], onlineSummary[,4], offlineSummary_orig, numAngles = 3, k=3)
kable(weight_ex[1:5,], caption="Table 3: Weight Vectors for k=3", row.names = FALSE)
```

```{r wtd_nn, include=FALSE, echo=FALSE, cache=TRUE}

# weighted prediction function in total
predXYwtd = function(newSignals, newAngles, trainData, 
                     numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)
    base = findWtdNN(newSignal = as.numeric(newSignals[i, ]), trainSS) # get matrix of x,y, numerator for weights
    wts = append(base[1:k, 4]/sum(base[1:k, 4]), rep(0, nrow(base)-k))  # calculate weights based on K, append zero array for delta of len-k
    base[, 2:3] = base[, 2:3]*wts # multiply weights array * matrix of x,y to get weighted vals
    closeXY[[i]] = base[,1:3] # append weighted xy, x, y values to list
  }
  estXY = lapply(closeXY, # loop over each xy position-based dataframe
                 function(x) sapply(x[ , 2:3], function(x) sum(x))) # sum all as neighbors > k == 0 now, and x,y is already weighted!
  estXY = do.call("rbind", estXY) # pull predictions together for each observation xy in test set
  return(estXY)
}

```

In summary, each new observation we wish to predict is compared to the training data set given the new observation's orientation. This is the "lazy learning" for which *k*-nearest neighbors is known. A model is never fit to the data, we simply utilize the existing training data for prediction. A unique weight vector is formed based on the number of neighbors *k* and position predictions are obtained by multiplying and summing these weights for each training observation's (x,y) position. 

Nolan and Temple Lang's mean-based method considers the mean of neighbors for prediction. This method can lead to skewness in predictions as averages are not resistant to outliers. Weighting neighbors, on the other hand, partially corrects for the skewness seen using averages.

*Nearest Neighbor Method Comparisons*

We formally compare our weighted *k*-nearest neighbors method to Nolan and Temple Lang's mean-based method. We cross-validate using 11 folds, each with 15 unique (x,y) positions. We consider the same training data sets used in the MAC analysis in the previous section.

```{r wtd_investigation, include=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}
results_wtd_orig = cross_validate(offline_orig, offlineSummary_orig, K=20, wtd=TRUE)
print(paste('results of weighted', results_wtd_orig))
```

We first consider a training data set containing the access point with a MAC address ending in 39:e1:c0 and excluding the access point with a MAC address ending in 39:dd:cd. This is the same training data Nolan and Temple Lang used for their model. It was previously seen that this mean-based *k*-nearest neighbors model resulted in an optimal root mean square error of 1038.5 using six neighbors.

When a weighted implementation is considered, the optimal *k*-nearest neighbors model produces a root mean square error of 1015.19 using six neighbors. This error is lower than the mean-based *k*-nearest neighbors model, indicating weighting outperforms the mean-based approach for the original training data.

Using the weighted implementation and six neighbors, errors are visualized as lines running from actual positions in the test data set to predicted positions from the *k*-nearest neighbors model. The asterisks represent predictions, black circles display the location of new observations from the test data set, and gray circles represent the positions of each observation in the training data. The model struggles with predicting locations in the southwest corner of the room. However, other predictions are close to actual observations from the test data set.

```{r viz_floor, include=TRUE, cache=TRUE, echo=FALSE, fig.cap=cap}

estXYk6wtd = predXYwtd(newSignals = onlineSummary_orig[ , 6:11], 
                       newAngles = onlineSummary_orig[ , 4], 
                       offlineSummary_orig, numAngles = 3, k = 3)
  
# show errors on the floor
floorErrorMap = function(estXY, actualXY, trainPoints = NULL, AP = NULL){
  
    plot(0, 0, xlim = c(0, 35), ylim = c(-3, 15), type = "n",
         xlab = "", ylab = "", axes = FALSE)
    box()
    if ( !is.null(AP) ) points(AP, pch = 15)
    if ( !is.null(trainPoints) )
    points(trainPoints, pch = 19, col="grey", cex = 0.6)
    
    points(x = actualXY[, 1], y = actualXY[, 2], 
           pch = 19, cex = 0.8 )
    points(x = estXY[, 1], y = estXY[, 2], 
           pch = 8, cex = 0.8 )
    segments(x0 = estXY[, 1], y0 = estXY[, 2],
             x1 = actualXY[, 1], y1 = actualXY[ , 2],
             lwd = 2, col = "red")
}


trainPoints = offlineSummary_orig[ offlineSummary_orig$angle == 0 & 
                                   offlineSummary_orig$mac == "00:0f:a3:39:e1:c0" ,
                                   c("posX", "posY")]

oldPar = par(mar = c(1, 1, 1, 1))

floorErrorMap(estXYk6wtd, onlineSummary_orig[ , c("posX","posY")], 
              trainPoints = trainPoints, AP = AP)

par(oldPar)

cap <- "Figure 5: Floor Level Errors Using Weighted KNN with 6 Neighbors"
```

We also analyze the dataset including the access point with a MAC address ending in 39:dd:cd and excluding the access point with a MAC address ending in 39:e1:c0. This data set matches the second data set analyzed in the previous section. Model performance was greatest using this data set and considering an mean-based *k*-nearest neighbors model.

```{r wtd_investigation_new, include=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}
results_wtd_new = cross_validate(offline_new, offlineSummary_new, K=20, wtd=TRUE)
print(paste('results of weighted', results_wtd_new))
```

Cross-validation of a weighted *k*-nearest neighbors model on the adjusted training data is executed using 11 folds and 15 unique (x,y) positions. The optimal model uses six nearest neighbors and results in a root mean square error of 908.38. This result is better than both the mean-based *k*-nearest neighbors model as well as the weighted model considering the alternate access point.

```{r wtd_combined, include=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}

reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 7,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

results_combined_wtd = cross_validate(offline, offlineSummary, K=20, wtd=TRUE)
print(paste('results of combined',results_combined_wtd))
```

For completeness, both access points are considered in a weighted *k*-nearest neighbors model. After cross-validation, minimum root mean square error of 1051.74 is achieved with four neighbors. Similar to the mean-based model, including both access points in the weighted model results in the highest error.

## Results

### MAC Address Conclusions

Given the learning curves displayed in Figure 4, including the access point with a MAC address of 00:0f:a3:39:dd:cd results in the lowest error. This result implies Nolan and Temple Lang may have discarded the incorrect access point. However, we utilize a random sample with a set seed to develop our cross-validation folds. Given the relative closeness of error in the displayed results and randomness involved in developing the cross-validation folds, the decision to *exclude* 00:0f:a3:39:dd:cd could easily become the superior option. This was the case when considering other cross-validation folds and can be confirmed by an eager reader.

```{r compare_access_points, include=TRUE, cache=TRUE, echo=FALSE, fig.cap=cap}
ggplot()+
  geom_line(mapping=aes(x=seq(1,20,1), y=results[[3]], color="00:0f:a3:39:e1:c0"), show.legend = TRUE)+
  geom_line(mapping=aes(x=seq(1,20,1), y=results_new[[3]], color="00:0f:a3:39:dd:cd"), linetype="dashed", show.legend = TRUE)+
  geom_line(mapping=aes(x=seq(1,20,1), y=results_combined[[3]], color="Both APs"), linetype="dotdash", show.legend = TRUE)+
  theme(legend.position="top", legend.text = element_text(colour="black", size=8, 
                                                          face="bold"))+
  theme(legend.position="top", 
        legend.text = element_text(colour="black", size=8, face="bold"),
        legend.title=element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")
        )+
  labs(y="error", x="k")

cap <- "Figure 6: Learning Curves for Each Scenario Given K Neighbors"
```

Nolan and Temple Lang's decision to use the MAC Address of 00:0f:a3:39:e1:c0 and exclude the address of 00:0f:a3:39:dd:cd in their location analysis may have been predicated on knowledge of the hardware of the two devices and some difference in the performance. The signal strength visualization seems to lend credence to this idea. Visualizing a larger area of higher signal strength offered by dd:cd might suggest a lack of sensitivity between measurement of signal strength at different position if the strength of the signal decays more slowly or operates generally at a higher strength.

The failure of the inclusion of both MAC Addresses to improve error versus the inclusion of only the address of the access point ending in 39:e1:c0 might suggest that the access points need to be dispersed, and generally lower powered if any gain in accuracy is to be gained from adding access points.

This might offer another line of investigation to deterimine the optimal coverage model for a desired confidence level given a position. While it could be assumed that more access points will generally improve the performance of the location system, the placement of each access point is very important in managing the system. If this access point was placed elsewhere in the environment, the results would likely have been different.

### Weighted *k*-nearest Neighbors Conclusions

To compare the two *k*-nearest neighbors model implementations for each training data set scenario, we provide a learning curve for each scenario in Figure 5.

```{r compare_methods, include=TRUE, cache=TRUE, echo=FALSE, fig.cap=cap}
ggplot(mapping=aes(x=seq(1,20,1)))+
  geom_line(mapping=aes(y=results[[3]], color="red"), linetype="solid")+
  geom_line(mapping=aes(y=results_new[[3]], color="blue"), linetype="solid")+
  geom_line(mapping=aes(y=results_combined[[3]], color="black"), linetype="solid")+
  geom_line(mapping=aes(y=results_wtd_orig[[3]], color="red"), linetype="dashed")+
  geom_line(mapping=aes(y=results_wtd_new[[3]], color="blue"), linetype="dashed")+
  geom_line(mapping=aes(y=results_combined_wtd[[3]], color="black"), linetype="dashed")+
  theme(legend.position="top", 
        legend.text = element_text(colour="black", size=8, face="bold"),
        legend.title=element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")
        )+
  scale_color_discrete(name = "MAC", labels = c("Include 39:e1:c0", "Include 39:dd:cd", "Include Both"))+
  labs(y="error", x="k")

cap <- "Figure 7: Learning Curves for Each Scenario Given K Neighbors"
```

The dotted lines represent error results from weighted *k*-nearest neighbor models for each training set scenario analyzed. Solid lines represent mean-based *k*-nearest neighbor models. It is easily noticeable that the weighted implementations outperform for each scenario, especially when considering a larger number of neighbors. 

This result makes logical sense, as the weighted method places minimal emphasis on neighbors further away. The average method considers each neighbor equally and is severely affected by skewness. Therefore, one outlier neighbor can influence the predicted position drastically in an mean-based *k*-nearest neighbors model.

In general, we consider the weighted *k*-nearest neighbors method more efficient. Especially when considering observations that are not tightly clustered, as in our example.

## Conclusions and Future Work

We have shown the results of two location determination algorithms using different data sets. For our case, there were multiple factors that changed the performance of our indoor positioning system. Within the limited example of a single floor of a multi-story building, we saw a difference in accuracy through the inclusion or exclusion of an overlapping access point. Further, different location determination algorithms offered different results. Exploration of the visualization of several access points showed obstructions changed the decay of signal strength. When developing an indoor positioning system, it is critical to assess environmental surroundings, hardware and location determination algorithms. These three factors are unique for every indoor positioning system. A general framework should not be applied if optimal performance is expected.

The ever-increasing availability of public Wi-Fi has helped RTLS expand its reach. Wireless devices are built using similar manufacturing standards. However, these devices have a vast array of functionality. Therefore, it is important to understand the location accuracy differences that exist between tracking a user with a tablet, a phone, or a laptop. Further, environmental surroundings greatly impact the ability of locations to be predicted. Building materials or objects in the line of sight of a mobile tag can affect the accuracy of a signal and subsequently lead to mistargeting a location. Real-world environments vary drastically. While our case used a single mobile device, a real-world application may need to provide a location for across many different types of devices.

Real-time location systems can result in privacy concerns given the high level of precision offered with modern location determination technology. When a device is used to track a piece of equipment, one could surmise that the user of the equipment will generally be in possession of the device. This location then serves to track to the user throughout their work day. A recent patent issued to Amazon is for a wrist-wearable device that tracks a warehouse employee's hands in real-time (Ong 13). Employees assume that the device is used as a measure undertaken to reduce errors in the picking, processing and shipping of customer orders. However, the employee can also view the device as an overlord, creating a running case for justifiable termination based on comparative productivity against other workers. Employees would likely not see an equal access to this data for their own cause. A company implementing RTLS employee trackers would likely protect the data against subpoena by binding arbitration agreements in the case of an on-the-job injury. Encryption may offer an additional measure of security and privacy protection. Yi, et al. propose to utilize the Paillier public-key cryptosystem during the query of the location given a *k*-nearest neighbors model. This provides the end-user with both location and enhances data privacy (Yi, et al 14). 

Real-time location services in retail could serve to reduce the intrusiveness of market-research activities conducted in stores. Firms, including VideoMining, have long used in-store video surveillance to monitor shoppers' activities in store. While most consumers would generally expect that their activities in public are subject to observation, few shoppers might consider the existence of firms that will review footage and analyze the behavior of individual shoppers from the moment they enter the store. This analysis includes classification of the shopper based on their observed and perceived demographics (Hudson 15). Use of a RTLS tag on the shopping cart or basket could bring more anonymity to the process and eliminate the use of the video footage in this activity.

Recent advances in RTLS usage have been seen in the healthcare industry. Patient Room 2020 is a project led by the American Medical Association that uses RTLS to reduce inefficiencies (Bridges 16). The software tools help the administrators cut costs without compromising patient care or safety. The positioning system tracks a caregivers' position to check on wait times and response times to the patients. RTLS can help identify efficiency gaps and workflow bottlenecks. An administrator can easily identify the closest caregiver to a patient room which clearly has an impact on operational efficiencies. However, given HIPAA regulations, data created by a real-time location system should be heavily scrutinized. It is important to promote benefits of RTLS and provide transparency to how the employer or system owner will use the data, as well as how the employer will safeguard and restrict the use or release of the data.

While real-time data collection is far from new, the public has little understanding of data collection processes. One could argue that the public's understanding of real-time location services will have a similarly long learning curve. As noted already, many areas of concern for privacy and data security already exist. An ethical framework is required for RTLS data as with all data collection practices. This framework should be based on the foundation of transparency to ensure efficient public understanding of data collection and retention policies.


## References

1. Nolan, D., Temple Lang, D. DATA SCIENCE IN R: a Case Studies Approach to Computational Reasoning and Problem Solving. CRC PRESS, 2017.

2. Ojo-Osagie, O. “Dells Just In Time Inventory Management System.” Academia.edu, www.academia.edu/23256794/Dells_Just_In_Time_Inventory_Management_system.

3. Winick, E. “Amazon's Investment in Robots Is Eliminating Human Jobs.” MIT Technology Review, 4 Dec. 2017, www.technologyreview.com/the-download/609672/amazons-investment-in-robots-is-eliminating-human-jobs/.

4. ISO Committee. “Arameters for Air Interface Communications at 860 MHz to 960 MHz General.” International Organization for Standardization, 15 Jan. 2018, www.iso.org/standard/59644.html.

5. Barker, P., et al. "Performance modelling of the IrDA infrared wireless communications protocol stack." International Journal of Communication Systems, 2000. https://aetos.it.teithe.gr/~vitsas/publications/IntJCommSys_Peter.pdf

6. Boulos, Maged N Kamel, and Geoff Berry. “Real-Time Locating Systems (RTLS) in Healthcare: a Condensed Primer.” International Journal of Health Geographics, BioMed Central, 28 June 2012, www.ncbi.nlm.nih.gov/pmc/articles/PMC3408320/.

7. Madigan, D. et al. "Location Estimation in Wireless Networks: A Bayesian Approach." Rutgers University and Avaya Labs, 2006. http://dimacs.rutgers.edu/Research/MMS/PAPERS/wireless.ps.

8. Tarrío, P., et al. "Weighted Least Squares Techniques for Improved Received Signal Strength Based Localization." Sensors, 2011. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3231493/

9. Swedberg, C. “Toronto General Hospital Uses RTLS to Reduce Infection Transmission.” RFID Journal, 28 Feb. 2012, www.rfidjournal.com/articles/view?9266.

10. Chung, Won. “RTLS and Livestock Management System.” Google Patents, Google, 10 Sept. 2010, patents.google.com/patent/US20120065483A1/en.

11. Hidalgo, Jeff. “Raytheon Missile Facility Modernized by Advanced Technologies.” Raytheon Company: Technology Today , 2015, www.raytheon.com/news/technology_today/2015_i1/facilitymod.html.

12. Ding, Bin, et al. “Application of RTLS in Warehouse Management Based on RFID and Wi-Fi.” IEEE Xplore, 12 Oct. 2008, ieeexplore.ieee.org/abstract/document/4679157/.

13. Ong, Thuy. “Amazon Patents Wristbands That Track Warehouse Employees' Hands in Real Time.” The Verge, The Verge, 1 Feb. 2018, www.theverge.com/2018/2/1/16958918/amazon-patents-trackable-wristband-warehouse-employees.

14. Yi, Xun; Paulet, Russel; Bertino, Elisa; Varadharajan, Vijay. "Practical *k*-nearest Neighbor Queries with Location Privacy" (2014). Cyver Center Publications.  Paper 592.  Retrieved from http://dx.doi.org/10.1109/ICDE.2014.6816688.

15. Hudson, Steve. “How Retailers Can Extract Value From Location Data.” RFID Journal, 11 Jan. 2016, www.rfidjournal.com/articles/view?13922.

16. Bridges, Holly. “Patient Room 2020 Is on Its Way. More Calming, Connected Care.” Innovatemedtec, 19 May 2015, innovatemedtec.com/content/patient-room-2020-is-on-its-way-more-calming-connected-care.

```{r data_prep, eval=FALSE, include=FALSE, echo=FALSE}
# This is the code from Nolan and Lang Chapter 1
# this chunk has been moved to a makefile for reproducible research purposes
options(digits = 2)


# Read in the text file to a matrix called txt
txt = readLines("Data/offline.final.trace.txt")

# get a count of lines that have a # in the first character location 
sum(substr(txt, 1, 1) == "#")

# total lines in the txt matrix
length(txt)

# as a test, strsplit the 4th line on semicolons, print to screen
strsplit(txt[4], ";")[[1]]

# store results of string of the 4th line to a vector
tokens = strsplit(txt[4], "[;=,]")[[1]]

# print tokens 1:10 to screen
tokens[1:10]

# extract the 2nd, 4th, 6:8 and 10th elements to the screen
# these tokens make up the handheld device
tokens[c(2, 4, 6:8, 10)]

# get features for readings (all else not == handheld device)
tokens[ - ( 1:10 ) ]

# assign a matrix of everything except elements 1:10 to a 4 coloumn matrix called tmp
# build a matrix for each section of the data and bind them together using cbind
tmp = matrix(tokens[ - (1:10) ], ncol = 4, byrow = TRUE) # get all signals
mat = cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), # repeat handheld device data
                   ncol = 6, byrow = TRUE), 
            tmp)
#print shape of mat to screen
dim(mat)

# make this extendable, create function to repeat process for each line in dataset
# this will create individual matrices for each line in the data (not efficient)
processLine =
function(x)
{
  tokens = strsplit(x, "[;=,]")[[1]]
  tmp = matrix(tokens[ - (1:10) ], ncol = 4, byrow = TRUE)
  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),
               ncol = 6, byrow = TRUE), tmp)
}

# apply to 17 lines of first file at x,y = 0,0 and pos = 0.0
# creates 17 matrices, returns them in a list
tmp = lapply(txt[4:20], processLine)

# count the number of records in each matrix created
# use sapply (preserves dimensions tmp) to show the number of rows in each row in tmp
sapply(tmp, nrow)

# stack the matrices using rbind and do.call
offline = as.data.frame(do.call("rbind", tmp))
dim(offline)

# now do this for the entire dataset
# create a list called lines that contains the lines that don't start with a # 
lines = txt[ substr(txt, 1, 1) != "#" ]
tmp = lapply(lines, processLine)


# adjust induction function for issue where no readings for a position x,y and orientation
# discard if we only find tokens for handheld device
processLine = function(x)
{
  tokens = strsplit(x, "[;=,]")[[1]]
  
  if (length(tokens) == 10)  # this is the adjustment, checks to see if we have signal responses, if not, returns null
    return(NULL)
 
  tmp = matrix(tokens[ - (1:10) ], ncol= 4, byrow = TRUE)
  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6, 
               byrow = TRUE), tmp)
}

options(error = recover, warn = 1)
tmp = lapply(lines, processLine)
offline = as.data.frame(do.call("rbind", tmp), 
                        stringsAsFactors = FALSE)

# offline dataframe in tidy format with readings down instead of across
dim(offline)

# clean data and build represenation for analysis

# rename the columns, transform numeric values
names(offline) = c("time", "scanMac", "posX", "posY", "posZ", 
                   "orientation", "mac", "signal", 
                   "channel", "type")

numVars = c("time", "posX", "posY", "posZ", 
            "orientation", "signal")
offline[ numVars ] =  lapply(offline[ numVars ], as.numeric)

# drop all vars with 1 as mode and remove type variable
# only want to keep access point (type 3)
offline = offline[ offline$type == "3", ]
offline = offline[ , "type" != names(offline) ] # this filters out type from columns
dim(offline)

# transform time from ms to s in order to get POSIXt format
offline$rawTime = offline$time
offline$time = offline$time/1000 # convert from ms to s
class(offline$time) = c("POSIXt", "POSIXct")

# check the class of each variable in our dataset
unlist(lapply(offline, class))

# summary stats on our numeric vars
summary(offline[, numVars])

# only one MAC for our scanning device: 00:02:2D:21:0F:33
summary(sapply(offline[ , c("mac", "channel", "scanMac")],
                as.factor))

# remove scanMac and posZ from offline df: no posZ in data and only one scanMac
offline = offline[ , !(names(offline) %in% c("scanMac", "posZ"))]

# exploring orientation, far more than 8 values for orientation
length(unique(offline$orientation))

# values distributed in clusters around what we'd expect
plot(ecdf(offline$orientation))

# build pdf export
#pdf(file = "Images/Geo_ECDFOrientation.pdf", width = 10, height = 7)
oldPar = par(mar = c(4, 4, 1, 1))
plot(ecdf(offline$orientation), pch = 19, cex = 0.3,
     xlim = c(-5, 365), axes = FALSE,
     xlab = "orientation", ylab = "Empirical CDF", main = "")
box()
axis(2)
axis(side = 1, at = seq(0, 360, by = 45))
par(oldPar)
#dev.off()


#pdf(file = "Images/Geo_DensityOrientation.pdf", width = 10, height = 5)
oldPar = par(mar = c(4, 4, 1, 1))
plot(density(offline$orientation, bw = 2), 
 xlab = "orientation", main = "")
par(oldPar)
#dev.off()

# to normalize, build function to round off orientation
roundOrientation = function(angles) {
  refs = seq(0, by = 45, length  = 9)
  q = sapply(angles, function(o) which.min(abs(o - refs))) # which angle is orientation value closest to? get index
  c(refs[1:8], 0)[q] # map index to reference positions, taking care to make sure 360 == 0
}

offline$angle = roundOrientation(offline$orientation)

# boxplot showing transforms of orientation to nearest 45 deg angle
with(offline, boxplot(orientation ~ angle,
                      xlab="nearest 45 degree angle",
                      ylab="orientation"))

#pdf(file = "Images/Geo_BoxplotAngle.pdf", width = 10)
with(offline, boxplot(orientation ~ angle,
                      xlab="nearest 45 degree angle",
                      ylab="orientation"))
oldPar = par(mar = c(4, 4, 1, 1))
par(oldPar)
#dev.off()


# Exploring MAC Addresses
c(length(unique(offline$mac)), length(unique(offline$channel)))

# check counts of observationsf or MAC addresses
table(offline$mac)

# obviously extra MAC addresses, some with not many readings, get rid of those
# keep records from 7 top read devices (one more than we need)
subMacs = names(sort(table(offline$mac), decreasing = TRUE))[1:7]

# keep rows with subMacs identified only, discarding others in training set
offline = offline[ offline$mac %in% subMacs, ]

# create a table of counts for mac and channel and filter it
macChannel = with(offline, table(mac, channel))
apply(macChannel, 1, function(x) sum(x > 0))
# one unique channel per MAC

# eliminate channel from offline dataset, not necessary for analysis
offline = offline[ , "channel" != names(offline)]
```

```{r data_prep_func, eval=FALSE, include=FALSE, echo=FALSE}
# EXPLORING POSITION OF HANDHELD DEVICE
# this chunk has been moved to a makefile for reproducible research purposes

# how many unique x,y positions?
locDF = with(offline, 
             by(offline, list(posX, posY), function(x) x))
length(locDF)
# way too many, > 166 quoted

# which locations are empty: 310
sum(sapply(locDF, is.null))

# remove nulls and we get 166 unique x,y locs, good to go
locDF = locDF[ !sapply(locDF, is.null) ]
length(locDF)

# get counts by x,y position, should be ~5300 110 readings * 8 angles * 6 positions 
locCounts = sapply(locDF, nrow)

locCounts = sapply(locDF, 
                   function(df) 
                     c(df[1, c("posX", "posY")], count = nrow(df)))

class(locCounts)

dim(locCounts)

locCounts[ , 1:8]

locCounts = t(locCounts)
plot(locCounts, type = "n", xlab = "", ylab = "")
text(locCounts, labels = locCounts[,3], cex = 0.8, srt = 45)

#pdf(file = "Images/Geo_XYByCount.pdf", width = 10)
oldPar = par(mar = c(3.1, 3.1, 1, 1))

locCounts = t(locCounts)
plot(locCounts, type = "n", xlab = "", ylab = "")
text(locCounts, labels = locCounts[,3], cex = .8, srt = 45)

par(oldPar)
#dev.off()


# CREATING A FUNCTION TO PREPARE THE DATA
readData = 
  function(filename = 'Data/offline.final.trace.txt', 
           subMacs = c("00:0f:a3:39:e1:c0", "00:0f:a3:39:dd:cd", "00:14:bf:b1:97:8a",
                       "00:14:bf:3b:c7:c6", "00:14:bf:b1:97:90", "00:14:bf:b1:97:8d",
                       "00:14:bf:b1:97:81"))
  {
    txt = readLines(filename)
    lines = txt[ substr(txt, 1, 1) != "#" ]
    tmp = lapply(lines, processLine)
    offline = as.data.frame(do.call("rbind", tmp), 
                            stringsAsFactors= FALSE) 
    
    names(offline) = c("time", "scanMac", 
                       "posX", "posY", "posZ", "orientation", 
                       "mac", "signal", "channel", "type")
    
     # keep only signals from access points
    offline = offline[ offline$type == "3", ]
    
    # drop scanMac, posZ, channel, and type - no info in them
    dropVars = c("scanMac", "posZ", "channel", "type")
    offline = offline[ , !( names(offline) %in% dropVars ) ]
    
    # drop more unwanted access points
    offline = offline[ offline$mac %in% subMacs, ]
    
    # convert numeric values
    numVars = c("time", "posX", "posY", "orientation", "signal")
    offline[ numVars ] = lapply(offline[ numVars ], as.numeric)

    # convert time to POSIX
    offline$rawTime = offline$time
    offline$time = offline$time/1000
    class(offline$time) = c("POSIXt", "POSIXct")
    
    # round orientations to nearest 45
    offline$angle = roundOrientation(offline$orientation)
      
    return(offline)
  }

offlineRedo = readData()

# check for equality
identical(offline, offlineRedo)

# what variables did we use as global? these our are dependencies
library(codetools)
findGlobals(readData, merge =FALSE)$variables
```

```{r eda, eval=FALSE, include=FALSE, echo=FALSE}
# overall statistics for signal, range of -98 to -25 (lower is stronger)
summary(offline$signal)

#pdf(file = "Images/Geo_DensitySignalByMacAngle.pdf", width = 8, height = 12)
oldPar = par(mar = c(3.1, 3, 1, 1))

# investigate distribution of signals for one x,y & each angle
# non-normality and dual-modal apparent, e.g. 00:14:bf:b1:97:8a, at 270 deg orientation
densityplot( ~ signal | mac + factor(angle), 
             data = offline,
             subset = posX == 24 & posY == 4 & 
                         mac != "00:0f:a3:39:dd:cd",
             bw = 0.5, plot.points = FALSE)

par(oldPar)
#dev.off()

#offline = offline[ offline$mac != "00:0f:a3:39:dd:cd", ]

# create table of summary stats by Location, Angle and AP
offline$posXY = paste(offline$posX, offline$posY, sep = "-") # concat x,y

byLocAngleAP = with(offline, 
                    by(offline, list(posXY, angle, mac), 
                       function(x) x))

# calculate summary statistics, reduce down to single summary line
signalSummary = 
  lapply(byLocAngleAP,            
         function(oneLoc) {
           ans = oneLoc[1, ]
           ans$medSignal = median(oneLoc$signal)
           ans$avgSignal = mean(oneLoc$signal)
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
           })

# bind all the summary lines together
offlineSummary = do.call("rbind", signalSummary)     

#pdf(file = "Images/Geo_BoxplotSignalSDByAvg.pdf", width = 10)
oldPar = par(mar = c(3.1, 3, 1, 1))

# create box and whisker for standard deviation for each avg Signal calculated in summary
# shows us how variable signal ranges can be
# more variable the stronger the signal is
breaks = seq(-90, -30, by = 5)
bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),
       data = offlineSummary, 
       subset = mac != "00:0f:a3:39:dd:cd",
       xlab = "Mean Signal", ylab = "SD Signal")

par(oldPar)
#dev.off()


#pdf(file = "Images/Geo_ScatterMean-Median.pdf", width = 10)
oldPar = par(mar = c(4.1, 4.1, 1, 1))

# examine skewness of signal strength (avg - med signal) vs num observations
# if normal should be higher densities in center
with(offlineSummary,
     smoothScatter((avgSignal - medSignal) ~ num,
                   xlab = "Number of Observations", 
                   ylab = "mean - median"))
abline(h = 0, col = "#984ea3", lwd = 2)

# fit LOESS line, shows that there is not a lot of difference or trend
# skewness is minimal
lo.obj = 
  with(offlineSummary,
       loess(diff ~ num, 
             data = data.frame(diff = (avgSignal - medSignal),
                               num = num)))

lo.obj.pr = predict(lo.obj, newdata = data.frame(num = (70:120)))
lines(x = 70:120, y = lo.obj.pr, col = "#4daf4a", lwd = 2)

par(oldPar)
#dev.off()
 

# THE RELATIONSHIP BETWEEN SIGNAL AND DISTANCE

# contour plot investigation
# one MAC and one orientaiton
oneAPAngle = subset(offlineSummary, 
                    mac == subMacs[5] & angle == 0)
# tps = thin plate splines, fit surface to signal strength values at observed locations
# must pass a Z here, which is our avg signal, this will reflect our color
smoothSS = Tps(oneAPAngle[, c("posX","posY")], 
               oneAPAngle$avgSignal)

vizSmooth = predictSurface(smoothSS)

plot.surface(vizSmooth, type = "C")

# add locations where measurements were taken
points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5)

# create a function for charting purposes, to include multiple contour plots
surfaceSS = function(data, mac, angle = 45) {
  require(fields)
  oneAPAngle = data[ data$mac == mac & data$angle == angle, ]
  smoothSS = Tps(oneAPAngle[, c("posX","posY")], 
                 oneAPAngle$avgSignal)
  vizSmooth = predictSurface(smoothSS)
  plot.surface(vizSmooth, type = "C", 
               xlab = mac, ylab = "", xaxt = "n", yaxt = "n")
  points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5) 
}

parCur = par(mfrow = c(2,2), mar = rep(1, 4))

# make 4 calls, shows corridor effect, signal strength of course strongest closest to AP
mapply(surfaceSS, mac = subMacs[ rep(c(5, 1,2), each = 2) ], 
       angle = rep(c(0, 135, 180), 2),
       data = list(data = offlineSummary))
 
par(parCur)

# the two similar macs are subMacs[1,2]
# remove similar MAC
offlineSummary = subset(offlineSummary, mac != subMacs[2])

# create a mtrix with relevant positions for six access points on floor plan
AP = matrix( c( 7.5, 6.3, 2.5, -.8, 12.8, -2.8,  
                1, 14, 33.5, 9.3,  33.5, 2.8),
            ncol = 2, byrow = TRUE,
            dimnames = list(subMacs[ -2 ], c("x", "y") ))

AP

# relationship between signal stregnth and distance from AP
# distances from locations of the device emitting vs access point receiving
diffs = offlineSummary[ , c("posX", "posY")] - 
          AP[ offlineSummary$mac, ] # ordered and indexed by offlineSummary

offlineSummary$dist = sqrt(diffs[ , 1]^2 + diffs[ , 2]^2)

xyplot(signal ~ dist | factor(mac) + factor(angle), 
       data = offlineSummary, pch = 19, cex = 0.3,
       xlab ="distance")

#pdf(file="Images/Geo_ScatterSignalDist.pdf", width = 7, height = 10)
oldPar = par(mar = c(3.1, 3.1, 1, 1))

xyplot(signal ~ dist | factor(mac) + factor(angle), 
       data = offlineSummary, pch = 19, cex = 0.3,
       xlab ="distance")
par(oldPar)
#dev.off()

```

```{r knn_work, eval=FALSE, include=FALSE, echo=FALSE}

# using only signals to APs from previously trained locations
# need to reformat data so signal strength is across columns

macs = unique(offlineSummary$mac)
# read test data
online = readData("Data/online.final.trace.txt", subMacs = macs)

# set up unique XY
online$posXY = paste(online$posX, online$posY, sep = "-")

# 60 unique positions
length(unique(online$posXY))

# using only one orientation for each location
tabonlineXYA = table(online$posXY, online$angle)
tabonlineXYA[1:10, ]

# pivot data to include signals across columns
# take the mean of signal for each mac position along columns
keepVars = c("posXY", "posX", "posY", "orientation", "angle")
byLoc = with(online, 
             by(online, list(posXY), 
                function(x) {
                  ans = x[1, keepVars]
                  avgSS = tapply(x$signal, x$mac, mean)
                  y = matrix(avgSS, nrow = 1, ncol = 6,
                        dimnames = list(ans$posXY, names(avgSS)))
                  cbind(ans, y)
                }))

onlineSummary = do.call("rbind", byLoc)  

dim(onlineSummary)

# rename
names(onlineSummary)

# ANGLE ALIGNMENT
# want to find records in offline data (training) that have similar orientations to our new observations
# orientation CAN impact signal strength as already evidenced visually in boxplots

# finding nearest angles to help filter training data before aggregation:
m = 3; angleNewObs = 230
refs = seq(0, by = 45, length  = 8)
nearestAngle = roundOrientation(angleNewObs)
  
if (m %% 2 == 1) {
  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
} else {
  m = m + 1
  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  if (sign(angleNewObs - nearestAngle) > -1) 
    angles = angles[ -1 ]
  else 
    angles = angles[ -m ]
}
angles = angles + nearestAngle
angles[angles < 0] = angles[ angles < 0 ] + 360
angles[angles > 360] = angles[ angles > 360 ] - 360

# with desired angles, select observations from offlineSummary for training
offlineSubset = 
  offlineSummary[ offlineSummary$angle %in% angles, ]

# then reshape training set to match test set, with signals across columns
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY")) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

# build all of this into a funciton
trainSS = reshapeSS(offlineSubset, varSignal = "avgSignal")

# wrap code to select angles and reshape the training set
# takes angle of new observation and associated angles from dataset using m
# angleNewObs sets reference point
# m is the number of angles to keep between 1 and 5 around reference point
# signals is the training dataset to filter and reshape
selectTrain = function(angleNewObs, signals = NULL, m = 1){
  refs = seq(0, by = 45, length  = 8)
  nearestAngle = roundOrientation(angleNewObs)
  
  if (m %% 2 == 1) 
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {
    m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(angleNewObs - nearestAngle) > -1) 
      angles = angles[ -1 ]
    else 
      angles = angles[ -m ]
  }
  angles = angles + nearestAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles) 
  
  offlineSubset = signals[ signals$angle %in% angles, ]
  reshapeSS(offlineSubset, varSignal = "avgSignal")
}

# example
# use angle of 130 for offline summary taking 135 and two closest angles for training set
# reshape by aggregating, taking average of signals for each mac, angles considered
train130 = selectTrain(130, offlineSummary, m = 3)

head(train130)

length(train130[[1]])




# FINDING NEAREST NEIGHBORS
# takes numeric vector of 6 new signal strengths and result of selectTrain() training set
findNN = function(newSignal, trainSubset) {
  diffs = apply(trainSubset[ , 4:9], 1, 
                function(x) x - newSignal) # this inverts, places x,y on cols when called as.numeric
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) ) # this gets our distance (euclidean)
  closest = order(dists) # orders our distances in ascending
  return(trainSubset[closest, 1:3 ]) # returns the subset from training with closest distances, gives xy ID, x, y
}


# weighted nearest neighbors, returns numerator of weight 1/distance
findWtdNN = function(newSignal, trainSubset) {
  diffs = apply(trainSubset[ , 4:9], 1, 
                function(x) x - newSignal) 
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) ) 
  closest = order(dists) # orders distances ascending
  closeXY = trainSubset[closest, 1:3 ]
  weight = as.numeric(1/dists[closest]) # calculate numerator for weights, we'll filter these based on K in predXY
  return(cbind(closeXY, weight)) # add in numerator for our weights
}

# prediction using nearest neighbors from training set
predXY = function(newSignals, newAngles, trainData, 
                  numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles) # select training set based on angle of test obs, num of angles in proximity
    closeXY[[i]] = findNN(newSignal = as.numeric(newSignals[i, ]), trainSS) # find nearest neighbors, return closest in training set
  }
  estXY = lapply(closeXY, # loop over each xy position-based dataframe
                 function(x) sapply(x[ , 2:3], 
                                    function(x) mean(x[1:k]))) # take a simple average of x,y positions
  estXY = do.call("rbind", estXY) # pull predictions together for each observation xy in test set
  return(estXY)
}

# weighted prediction
predXYwtd = function(newSignals, newAngles, trainData, 
                     numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)
    base = findWtdNN(newSignal = as.numeric(newSignals[i, ]), trainSS) # get matrix of x,y, numerator for weights
    wts = append(base[1:k, 4]/sum(base[1:k, 4]), rep(0, nrow(base)-k))  # calculate weights based on K, append zero array for delta of len-k
    base[, 2:3] = base[, 2:3]*wts # multiply weights array * matrix of x,y to get weighted vals
    closeXY[[i]] = base[,1:3] # append weighted xy, x, y values to list
  }
  estXY = lapply(closeXY, # loop over each xy position-based dataframe
                 function(x) sapply(x[ , 2:3], function(x) sum(x))) # sum all as neighbors > k == 0 now, and x,y is already weighted!
  estXY = do.call("rbind", estXY) # pull predictions together for each observation xy in test set
  return(estXY)
}
```

```{r knnsummary, eval=FALSE, include=FALSE, echo=FALSE}

# this is testing on the test set... not optimal, validation is in the next section
# we are also using 3 angles to aggregate and average for our training set to compare to
estXYk1 = predXY(newSignals = onlineSummary[ , 6:11], 
                 newAngles = onlineSummary[ , 4], 
                 offlineSummary, numAngles = 3, k = 1)

estXYk3 = predXY(newSignals = onlineSummary[ , 6:11], 
                 newAngles = onlineSummary[ , 4], 
                 offlineSummary, numAngles = 3, k = 3)

estXYk3wtd = predXYwtd(newSignals = onlineSummary[ , 6:11], 
                 newAngles = onlineSummary[ , 4], 
                 offlineSummary, numAngles = 3, k = 3)

# with 7 neighbors
estXYk7 = predXY(newSignals = onlineSummary[ , 6:11], 
                 newAngles = onlineSummary[ , 4], 
                 offlineSummary, numAngles = 3, k = 7)

estXYk7wtd = predXYwtd(newSignals = onlineSummary[ , 6:11], 
                 newAngles = onlineSummary[ , 4], 
                 offlineSummary, numAngles = 3, k = 7)


actualXY = onlineSummary[ , c("posX", "posY")]

calcError = 
function(estXY, actualXY) 
   sum( rowSums( (estXY - actualXY)^2) )

sapply(list(estXYk1, estXYk3, estXYk3wtd), calcError, actualXY)

# testing without CV folds, full test set, takes about 2 minutes
# this is not proper error testing! 

error_mean_vec = c()
error_wtd_vec = c()

for (i in seq(1,20,2)){
  
       mean = predXY(newSignals = onlineSummary[ , 6:11], 
             newAngles = onlineSummary[ , 4], 
             offlineSummary, numAngles = 3, k = i)
       error_mean = calcError(mean, actualXY)
       cat("With",i,"Neighbors Summary:\n\nMean KNN Error", error_mean)
       error_mean_vec = append(error_mean_vec, error_mean)
       
       wtd = predXYwtd(newSignals = onlineSummary[ , 6:11], 
             newAngles = onlineSummary[ , 4], 
             offlineSummary, numAngles = 3, k = i)
       error_wtd = calcError(wtd, actualXY)
       cat("\nWeighted KNN Error", error_wtd)
       error_wtd_vec = append(error_wtd_vec, error_wtd)
       cat("\nWeighted KNN minus Mean KNN Error:", error_wtd - error_mean,"\n\n")
} 


# visualize learning curve based on square error, iterating over K
df = as_data_frame(cbind(error_mean_vec, error_wtd_vec, k=seq(1,20,2)))

#pdf(file = "Images/KNNvWTDKNNfull.pdf", width = 10)
oldPar = par(mar = c(4.1, 4.1, 1, 1))

# plot learning curve
library(ggplot2)
df %>%
ggplot()+
  geom_line(mapping=aes(x=k, y=error_mean_vec, color="mean KNN"), show.legend = TRUE)+
  geom_line(mapping=aes(x=k, y=error_wtd_vec, color="wtd KNN"), linetype="dashed", show.legend = TRUE)+
  ggtitle('Learning Curve given K')+
  labs(y="error")

par(oldPar)
#dev.off()


# show errors on the floor
floorErrorMap = function(estXY, actualXY, trainPoints = NULL, AP = NULL){
  
    plot(0, 0, xlim = c(0, 35), ylim = c(-3, 15), type = "n",
         xlab = "", ylab = "", axes = FALSE)
    box()
    if ( !is.null(AP) ) points(AP, pch = 15)
    if ( !is.null(trainPoints) )
      points(trainPoints, pch = 19, col="grey", cex = 0.6)
    
    points(x = actualXY[, 1], y = actualXY[, 2], 
           pch = 19, cex = 0.8 )
    points(x = estXY[, 1], y = estXY[, 2], 
           pch = 8, cex = 0.8 )
    segments(x0 = estXY[, 1], y0 = estXY[, 2],
             x1 = actualXY[, 1], y1 = actualXY[ , 2],
             lwd = 2, col = "red")
}

trainPoints = offlineSummary[ offlineSummary$angle == 0 & 
                              offlineSummary$mac == "00:0f:a3:39:e1:c0" ,
                        c("posX", "posY")]

#pdf(file="Images/GEO_FloorPlanK3Errors.pdf", width = 10, height = 7)
oldPar = par(mar = c(1, 1, 1, 1))

floorErrorMap(estXYk3, onlineSummary[ , c("posX","posY")], 
              trainPoints = trainPoints, AP = AP)
par(oldPar)
#dev.off()

#pdf(file="Images/GEO_FloorPlanK1Errors.pdf", width = 10, height = 7)
oldPar = par(mar = c(1, 1, 1, 1))
floorErrorMap(estXYk1, onlineSummary[ , c("posX","posY")], 
              trainPoints = trainPoints, AP = AP)
par(oldPar)
#dev.off()

#pdf(file="Images/GEO_FloorPlanK7Errors.pdf", width = 10, height = 7)
oldPar = par(mar = c(1, 1, 1, 1))
floorErrorMap(estXYk7, onlineSummary[ , c("posX","posY")], 
              trainPoints = trainPoints, AP = AP)
par(oldPar)
#dev.off()

#pdf(file="Images/GEO_FloorPlanK7WTDErrors.pdf", width = 10, height = 7)
oldPar = par(mar = c(1, 1, 1, 1))
floorErrorMap(estXYk7wtd, onlineSummary[ , c("posX","posY")], 
              trainPoints = trainPoints, AP = AP)
par(oldPar)
#dev.off()

```

```{r cross_validation, eval=FALSE, include=FALSE, echo=FALSE}
# cross validate over each location, using all 8 orientations and 6 MAC addresses
# each fold has 166/11 = 15 locations
# must randomly select
v = 11
# permute locations
permuteLocs = sample(unique(offlineSummary$posXY))

# to calculate folds, build a matrix with 11 columns and ~15 locations each
permuteLocs = matrix(permuteLocs, ncol = v, 
                     nrow = floor(length(permuteLocs)/v))

# get the first validation fold from our offline data
onlineFold = subset(offlineSummary, posXY %in% permuteLocs[ , 1])

# must re-summarize each fold to match onlineSummary format
# selecting orientation at random to create our CV folds
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

# exclude MAC
offline = offline[ offline$mac != "00:0f:a3:39:dd:cd", ]

keepVars = c("posXY", "posX","posY", "orientation", "angle")

# build CV base from offline data in general
onlineCVSummary = reshapeSS(offline, keepVars = keepVars, 
                            sampleAngle = TRUE)

# an example of one fold
onlineFold = subset(onlineCVSummary, 
                    posXY %in% permuteLocs[ , 1])

# this is our training set
offlineFold = subset(offlineSummary,
                     posXY %in% permuteLocs[ , -1])

# using both methods with k = 3
estFold = predXY(newSignals = onlineFold[ , 6:11], 
                 newAngles = onlineFold[ , 4], 
                 offlineFold, numAngles = 3, k = 3)

estFoldwtd = predXYwtd(newSignals = onlineFold[ , 6:11], 
                 newAngles = onlineFold[ , 4], 
                 offlineFold, numAngles = 3, k = 3)

actualFold = onlineFold[ , c("posX", "posY")]
calcError(estFoldwtd, actualFold)

# formally test K out to 20 neighbors
K = 20
err = rep(0, K)
err_wtd = rep(0,K) # weighted error

for (j in 1:v) {
  onlineFold = subset(onlineCVSummary, 
                      posXY %in% permuteLocs[ , j])
  offlineFold = subset(offlineSummary,
                       posXY %in% permuteLocs[ , -j])
  actualFold = onlineFold[ , c("posX", "posY")]
  
  for (k in 1:K) {
    estFold = predXY(newSignals = onlineFold[ , 6:11],
                     newAngles = onlineFold[ , 4], 
                     offlineFold, numAngles = 3, k = k)
    err[k] = err[k] + calcError(estFold, actualFold)
    
    estFold_wtd = predXYwtd(newSignals = onlineFold[ , 6:11], # add in weighted calculations
                     newAngles = onlineFold[ , 4], 
                     offlineFold, numAngles = 3, k = k)
    err_wtd[k] = err_wtd[k] + calcError(estFold_wtd, actualFold)
  }
}

pdf(file = "Images/Geo_CVChoiceOfK.pdf", width = 8, height = 6)
oldPar = par(mar = c(4, 3, 1, 1))
plot(y = err, x = (1:K),  type = "l", lwd= 2,
     ylim = c(0, 2100),
     xlab = "Number of Neighbors",
     ylab = "Sum of Square Errors")
lines(y=err_wtd, x=1:K, lty = 2, lwd=2, col='red')


rmseMin = min(err)
kMin = which(err == rmseMin)[1]

rmseMin_wtd = min(err_wtd)
kMin_wtd = which(err_wtd == rmseMin_wtd)[1]

```

```{r addtl_code, eval=FALSE, include=FALSE, echo=FALSE}
# additional code not in book:

segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), 
         lty = 2, lwd = 2)
segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, 
         col = grey(0.4), lty = 2, lwd = 2)

#mtext(kMin, side = 1, line = 1, at = kMin, col = grey(0.4))
text(x = kMin - 2, y = rmseMin + 40, 
     label = as.character(round(rmseMin)), col = grey(0.4))
par(oldPar)
dev.off()


estXYk5 = predXY(newSignals = onlineSummary[ , 6:11], 
                 newAngles = onlineSummary[ , 4], 
                 offlineSummary, numAngles = 3, k = 5)

calcError(estXYk5, actualXY)

predXY = function(newSignals, newAngles, trainData, 
                  numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)
    closeXY[[i]] = findNN(newSignal = as.numeric(newSignals[i, ]),
                           trainSS)
  }

  estXY = lapply(closeXY, function(x)
                            sapply(x[ , 2:3], 
                                    function(x) mean(x[1:k])))
  estXY = do.call("rbind", estXY)
  return(estXY)
}

```

